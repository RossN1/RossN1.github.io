<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Deploying Quantised LLMs | ross.</title>
<meta name="keywords" content="">
<meta name="description" content="In the first two posts, I explained why quantisation matters and how it works. Here I’ll cover how to deploy quantised large language models (LLMs) on edge hardware, what workflows are available, and what to measure in practice.

Deployment workflows
1. Hugging Face Transformers &#43; bitsandbytes (GPU or desktop)
This is the most common path if you have a consumer GPU. bitsandbytes integrates with Transformers to load models directly in 8-bit or 4-bit precision.">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/posts/deploying-quantised-llms/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.b489a6d73b9d5f542e10296b5fbfe24c07a73618c7ca7efba908fadedc6200f9.css" integrity="sha256-tImm1zudX1QuEClrX7/iTAenNhjHyn77qQj63txiAPk=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/deploying-quantised-llms/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="ross. (Alt + H)">ross.</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Deploying Quantised LLMs
    </h1>
    <div class="post-meta"><span title='2025-09-24 09:00:00 +0100 BST'>September 24, 2025</span>&nbsp;·&nbsp;3 min

</div>
  </header> 
  <div class="post-content"><p>In the first two posts, I explained why quantisation matters and how it works. Here I’ll cover how to deploy quantised large language models (LLMs) on edge hardware, what workflows are available, and what to measure in practice.</p>
<hr>
<h2 id="deployment-workflows">Deployment workflows<a hidden class="anchor" aria-hidden="true" href="#deployment-workflows">#</a></h2>
<h3 id="1-hugging-face-transformers--bitsandbytes-gpu-or-desktop">1. Hugging Face Transformers + bitsandbytes (GPU or desktop)<a hidden class="anchor" aria-hidden="true" href="#1-hugging-face-transformers--bitsandbytes-gpu-or-desktop">#</a></h3>
<p>This is the most common path if you have a consumer GPU. <code>bitsandbytes</code> integrates with Transformers to load models directly in 8-bit or 4-bit precision.</p>
<p>Example:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;meta-llama/Llama-2-7b-chat-hf&#34;</span>
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#f92672">=</span> AutoTokenizer<span style="color:#f92672">.</span>from_pretrained(model_name)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>bnb_config <span style="color:#f92672">=</span> BitsAndBytesConfig(load_in_4bit<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> AutoModelForCausalLM<span style="color:#f92672">.</span>from_pretrained(
</span></span><span style="display:flex;"><span>    model_name,
</span></span><span style="display:flex;"><span>    quantization_config<span style="color:#f92672">=</span>bnb_config,
</span></span><span style="display:flex;"><span>    device_map<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;auto&#34;</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>inputs <span style="color:#f92672">=</span> tokenizer(<span style="color:#e6db74">&#34;Hello world&#34;</span>, return_tensors<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pt&#34;</span>)<span style="color:#f92672">.</span>input_ids
</span></span><span style="display:flex;"><span>outputs <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>generate(inputs, max_new_tokens<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>)
</span></span><span style="display:flex;"><span>print(tokenizer<span style="color:#f92672">.</span>decode(outputs[<span style="color:#ae81ff">0</span>]))
</span></span></code></pre></div><p>This loads the 7B model in 4-bit precision. On a consumer GPU with 12–16 GB VRAM, that can be the difference between failing to load and running smoothly.</p>
<hr>
<h3 id="2-llamacpp--gguf-cpu-or-edge-devices">2. llama.cpp + GGUF (CPU or edge devices)<a hidden class="anchor" aria-hidden="true" href="#2-llamacpp--gguf-cpu-or-edge-devices">#</a></h3>
<p>For devices without a GPU, <code>llama.cpp</code> is a lightweight inference framework written in C++. It runs quantised models on CPU and supports Apple Silicon, ARM boards, and even mobile phones.</p>
<p>Steps:</p>
<ol>
<li>
<p><strong>Convert the model</strong>
Convert a Hugging Face checkpoint into GGUF format with chosen quantisation (e.g. Q4).</p>
<pre tabindex="0"><code>python convert-hf-to-gguf.py --model llama-2-7b --out llama-2-7b-q4.gguf
</code></pre></li>
<li>
<p><strong>Run inference</strong></p>
<pre tabindex="0"><code>./llama.cpp/llama-cli -m llama-2-7b-q4.gguf -p &#34;Explain quantisation in simple terms&#34;
</code></pre></li>
</ol>
<p>The GGUF format supports a range of quantisation types (Q2, Q4, Q8). Lower-bit variants reduce memory further but can affect quality.</p>
<hr>
<h3 id="3-onnx-runtime--tensorrt-accelerated-inference">3. ONNX Runtime / TensorRT (accelerated inference)<a hidden class="anchor" aria-hidden="true" href="#3-onnx-runtime--tensorrt-accelerated-inference">#</a></h3>
<p>For deployment on embedded GPUs or NPUs, ONNX Runtime and TensorRT both support quantised models. These are more common in production pipelines where integration with existing inference stacks is needed.</p>
<hr>
<h2 id="what-to-measure">What to measure<a hidden class="anchor" aria-hidden="true" href="#what-to-measure">#</a></h2>
<p>When deploying quantised models, I focus on four metrics:</p>
<ol>
<li><strong>Latency</strong>: tokens per second generated.</li>
<li><strong>Memory usage</strong>: peak RAM/VRAM during inference.</li>
<li><strong>Output quality</strong>: usually measured by perplexity or comparison against benchmarks.</li>
<li><strong>Power consumption</strong>: relevant on mobile or embedded hardware.</li>
</ol>
<p>A practical example:</p>
<ul>
<li>A 7B model in FP16 may run at ~10 tokens/sec on a laptop GPU and consume ~14 GB VRAM.</li>
<li>The same model in 4-bit quantisation can drop to ~3.5 GB VRAM and ~20 tokens/sec, with only a small drop in fluency.</li>
</ul>
<hr>
<h2 id="trade-offs-and-pitfalls">Trade-offs and pitfalls<a hidden class="anchor" aria-hidden="true" href="#trade-offs-and-pitfalls">#</a></h2>
<ul>
<li><strong>Accuracy loss</strong>: INT8 is usually safe; INT4 sometimes reduces fluency or coherence. Hybrid precision (e.g. embeddings in FP16, attention weights in INT4) can help.</li>
<li><strong>Hardware support</strong>: INT8 kernels are common, INT4 less so. llama.cpp handles this internally, but other runtimes may not.</li>
<li><strong>Calibration</strong>: PTQ benefits from representative input data to compute scales. Poor calibration increases error.</li>
<li><strong>Security risks</strong>: research has shown that quantisation can hide adversarial triggers in weights. While not a barrier to use, it’s a factor for production systems.</li>
</ul>
<hr>
<h2 id="future-directions">Future directions<a hidden class="anchor" aria-hidden="true" href="#future-directions">#</a></h2>
<p>Research is pushing quantisation further:</p>
<ul>
<li><strong>Mixed precision</strong>: combining FP16, INT8, and INT4 within a model.</li>
<li><strong>Outlier handling</strong>: methods like SpQR preserve rare but important weights in higher precision.</li>
<li><strong>Rotation-based schemes</strong>: SpinQuant aligns weight spaces for better quantisation.</li>
<li><strong>Integration with pruning and distillation</strong>: combining techniques to maximise efficiency.</li>
<li><strong>Hardware support</strong>: new accelerators are emerging with native INT4 instructions.</li>
</ul>
<hr>
<h2 id="closing-thoughts">Closing thoughts<a hidden class="anchor" aria-hidden="true" href="#closing-thoughts">#</a></h2>
<p>Quantisation is a trade-off: you exchange some model quality for a smaller footprint and faster execution. For edge deployment, it often makes the difference between “impossible” and “usable”. With careful choice of bit width, calibration, and tools, it’s possible to run models that once needed datacentres directly on laptops or embedded devices.</p>
<hr>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/posts/how-quantisation-works/">
    <span class="title">« Prev</span>
    <br>
    <span>How Quantisation Works: From FP32 to INT4</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">ross.</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
