<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on ross.</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Posts on ross.</description>
    <generator>Hugo -- 0.150.0</generator>
    <language>en-gb</language>
    <lastBuildDate>Wed, 24 Sep 2025 11:00:00 +0100</lastBuildDate>
    <atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Why Quantisation Matters for LLMs</title>
      <link>http://localhost:1313/posts/why-quantisation-matters-for-llms/</link>
      <pubDate>Wed, 24 Sep 2025 11:00:00 +0100</pubDate>
      <guid>http://localhost:1313/posts/why-quantisation-matters-for-llms/</guid>
      <description>&lt;p&gt;Large language models (LLMs) are powerful but expensive to run. A model with 7 billion parameters stored in FP32 requires around 28 GB of memory just for weights (7e9 × 4 bytes). For larger models such as LLaMA-70B, this rises into the hundreds of gigabytes. These numbers explain why LLMs usually run in datacentres on specialised GPUs with high memory bandwidth.&lt;/p&gt;
&lt;p&gt;If we want to deploy models on consumer hardware or edge devices, these requirements are too high. Laptops and phones rarely have more than a few gigabytes available for AI tasks, and embedded systems often have less.&lt;/p&gt;</description>
    </item>
    <item>
      <title>How Quantisation Works: From FP32 to INT4</title>
      <link>http://localhost:1313/posts/how-quantisation-works/</link>
      <pubDate>Wed, 24 Sep 2025 10:00:00 +0100</pubDate>
      <guid>http://localhost:1313/posts/how-quantisation-works/</guid>
      <description>&lt;p&gt;In the first post I explained why quantisation matters. Here I’ll focus on how it is applied, the main schemes in use, and some of the libraries that make it practical.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;precision-levels&#34;&gt;Precision levels&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;FP32&lt;/strong&gt;: full precision, used in training. High accuracy, but memory-intensive.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FP16 / bfloat16&lt;/strong&gt;: half precision. Often the default for training and inference on GPUs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;INT8&lt;/strong&gt;: 8-bit integers. Reduces memory by 4× compared to FP32.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;INT4&lt;/strong&gt;: 4-bit integers. More aggressive, but often still usable for inference.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each step down saves memory and bandwidth but increases quantisation error.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deploying Quantised LLMs</title>
      <link>http://localhost:1313/posts/deploying-quantised-llms/</link>
      <pubDate>Wed, 24 Sep 2025 09:00:00 +0100</pubDate>
      <guid>http://localhost:1313/posts/deploying-quantised-llms/</guid>
      <description>&lt;p&gt;In the first two posts, I explained why quantisation matters and how it works. Here I’ll cover how to deploy quantised large language models (LLMs) on edge hardware, what workflows are available, and what to measure in practice.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;deployment-workflows&#34;&gt;Deployment workflows&lt;/h2&gt;
&lt;h3 id=&#34;1-hugging-face-transformers--bitsandbytes-gpu-or-desktop&#34;&gt;1. Hugging Face Transformers + bitsandbytes (GPU or desktop)&lt;/h3&gt;
&lt;p&gt;This is the most common path if you have a consumer GPU. &lt;code&gt;bitsandbytes&lt;/code&gt; integrates with Transformers to load models directly in 8-bit or 4-bit precision.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
