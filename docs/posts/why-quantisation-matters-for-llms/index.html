<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Why Quantisation Matters for LLMs | ross.</title><meta name=keywords content><meta name=description content="Large language models (LLMs) are powerful but expensive to run. A model with 7 billion parameters stored in FP32 requires around 28 GB of memory just for weights (7e9 × 4 bytes). For larger models such as LLaMA-70B, this rises into the hundreds of gigabytes. These numbers explain why LLMs usually run in datacentres on specialised GPUs with high memory bandwidth.
If we want to deploy models on consumer hardware or edge devices, these requirements are too high. Laptops and phones rarely have more than a few gigabytes available for AI tasks, and embedded systems often have less."><meta name=author content><link rel=canonical href=https://rossn1.github.io/posts/why-quantisation-matters-for-llms/><link crossorigin=anonymous href=https://rossn1.github.io/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://rossn1.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://rossn1.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://rossn1.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://rossn1.github.io/apple-touch-icon.png><link rel=mask-icon href=https://rossn1.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://rossn1.github.io/posts/why-quantisation-matters-for-llms/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://rossn1.github.io/posts/why-quantisation-matters-for-llms/"><meta property="og:site_name" content="ross."><meta property="og:title" content="Why Quantisation Matters for LLMs"><meta property="og:description" content="Large language models (LLMs) are powerful but expensive to run. A model with 7 billion parameters stored in FP32 requires around 28 GB of memory just for weights (7e9 × 4 bytes). For larger models such as LLaMA-70B, this rises into the hundreds of gigabytes. These numbers explain why LLMs usually run in datacentres on specialised GPUs with high memory bandwidth.
If we want to deploy models on consumer hardware or edge devices, these requirements are too high. Laptops and phones rarely have more than a few gigabytes available for AI tasks, and embedded systems often have less."><meta property="og:locale" content="en-gb"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-09-24T11:00:00+01:00"><meta property="article:modified_time" content="2025-09-24T11:00:00+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Why Quantisation Matters for LLMs"><meta name=twitter:description content="Large language models (LLMs) are powerful but expensive to run. A model with 7 billion parameters stored in FP32 requires around 28 GB of memory just for weights (7e9 × 4 bytes). For larger models such as LLaMA-70B, this rises into the hundreds of gigabytes. These numbers explain why LLMs usually run in datacentres on specialised GPUs with high memory bandwidth.
If we want to deploy models on consumer hardware or edge devices, these requirements are too high. Laptops and phones rarely have more than a few gigabytes available for AI tasks, and embedded systems often have less."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://rossn1.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Why Quantisation Matters for LLMs","item":"https://rossn1.github.io/posts/why-quantisation-matters-for-llms/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Why Quantisation Matters for LLMs","name":"Why Quantisation Matters for LLMs","description":"Large language models (LLMs) are powerful but expensive to run. A model with 7 billion parameters stored in FP32 requires around 28 GB of memory just for weights (7e9 × 4 bytes). For larger models such as LLaMA-70B, this rises into the hundreds of gigabytes. These numbers explain why LLMs usually run in datacentres on specialised GPUs with high memory bandwidth.\nIf we want to deploy models on consumer hardware or edge devices, these requirements are too high. Laptops and phones rarely have more than a few gigabytes available for AI tasks, and embedded systems often have less.\n","keywords":[],"articleBody":"Large language models (LLMs) are powerful but expensive to run. A model with 7 billion parameters stored in FP32 requires around 28 GB of memory just for weights (7e9 × 4 bytes). For larger models such as LLaMA-70B, this rises into the hundreds of gigabytes. These numbers explain why LLMs usually run in datacentres on specialised GPUs with high memory bandwidth.\nIf we want to deploy models on consumer hardware or edge devices, these requirements are too high. Laptops and phones rarely have more than a few gigabytes available for AI tasks, and embedded systems often have less.\nQuantisation is one of the main ways to close this gap.\nWhat quantisation does Quantisation reduces the precision of numbers stored in the model. Instead of FP32, we use FP16, INT8, or INT4. This has three direct effects:\nSmaller memory footprint\nA 7B model at FP16 is ~14 GB. At INT8, it’s ~7 GB. At INT4, it drops to ~3.5 GB. These differences are the difference between “cannot load” and “runs on a single GPU or CPU”.\nFaster inference\nModern hardware executes low-precision arithmetic faster and moves less data through memory channels. INT8 or INT4 kernels can achieve higher throughput.\nLower power use\nMemory access is expensive in terms of energy. With fewer bytes to move, battery life on mobile or embedded devices improves.\nWhy edge deployment makes this important For cloud providers, the trade-off is between efficiency and accuracy. For edge devices, it’s often feasibility:\nMobile apps: chat assistants, transcription, or summarisation directly on a phone. IoT devices: local reasoning without sending data to the cloud. Privacy-sensitive settings: healthcare or finance, where keeping data local avoids exposure. Quantisation makes these scenarios realistic.\nThe cost: accuracy trade-offs Reducing precision introduces quantisation error. If a weight distribution has many outliers, compressing it to 4 bits can distort the range, leading to degraded fluency or higher perplexity. Accuracy loss depends on the scheme used and the model architecture.\nSome approaches leave sensitive layers in higher precision (embeddings, normalisation layers) while quantising others. Others combine quantisation with retraining so the model learns to be robust.\nOutlook Quantisation is not the only approach — pruning and distillation are also active research areas — but it is the most direct route to shrinking a model while keeping the same architecture. In the next post, I’ll break down how quantisation works in practice, the different schemes used, and the tools available today.\n","wordCount":"404","inLanguage":"en","datePublished":"2025-09-24T11:00:00+01:00","dateModified":"2025-09-24T11:00:00+01:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://rossn1.github.io/posts/why-quantisation-matters-for-llms/"},"publisher":{"@type":"Organization","name":"ross.","logo":{"@type":"ImageObject","url":"https://rossn1.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://rossn1.github.io/ accesskey=h title="ross. (Alt + H)">ross.</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://rossn1.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://rossn1.github.io/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Why Quantisation Matters for LLMs</h1><div class=post-meta><span title='2025-09-24 11:00:00 +0100 BST'>September 24, 2025</span>&nbsp;·&nbsp;2 min</div></header><div class=post-content><p>Large language models (LLMs) are powerful but expensive to run. A model with 7 billion parameters stored in FP32 requires around 28 GB of memory just for weights (7e9 × 4 bytes). For larger models such as LLaMA-70B, this rises into the hundreds of gigabytes. These numbers explain why LLMs usually run in datacentres on specialised GPUs with high memory bandwidth.</p><p>If we want to deploy models on consumer hardware or edge devices, these requirements are too high. Laptops and phones rarely have more than a few gigabytes available for AI tasks, and embedded systems often have less.</p><p><strong>Quantisation is one of the main ways to close this gap.</strong></p><h2 id=what-quantisation-does>What quantisation does<a hidden class=anchor aria-hidden=true href=#what-quantisation-does>#</a></h2><p>Quantisation reduces the precision of numbers stored in the model. Instead of FP32, we use FP16, INT8, or INT4. This has three direct effects:</p><ol><li><p><strong>Smaller memory footprint</strong></p><ul><li>A 7B model at FP16 is ~14 GB.</li><li>At INT8, it’s ~7 GB.</li><li>At INT4, it drops to ~3.5 GB.</li></ul><p>These differences are the difference between “cannot load” and “runs on a single GPU or CPU”.</p></li><li><p><strong>Faster inference</strong><br>Modern hardware executes low-precision arithmetic faster and moves less data through memory channels. INT8 or INT4 kernels can achieve higher throughput.</p></li><li><p><strong>Lower power use</strong><br>Memory access is expensive in terms of energy. With fewer bytes to move, battery life on mobile or embedded devices improves.</p></li></ol><hr><h2 id=why-edge-deployment-makes-this-important>Why edge deployment makes this important<a hidden class=anchor aria-hidden=true href=#why-edge-deployment-makes-this-important>#</a></h2><p>For cloud providers, the trade-off is between efficiency and accuracy. For edge devices, it’s often <strong>feasibility</strong>:</p><ul><li><strong>Mobile apps</strong>: chat assistants, transcription, or summarisation directly on a phone.</li><li><strong>IoT devices</strong>: local reasoning without sending data to the cloud.</li><li><strong>Privacy-sensitive settings</strong>: healthcare or finance, where keeping data local avoids exposure.</li></ul><p>Quantisation makes these scenarios realistic.</p><hr><h2 id=the-cost-accuracy-trade-offs>The cost: accuracy trade-offs<a hidden class=anchor aria-hidden=true href=#the-cost-accuracy-trade-offs>#</a></h2><p>Reducing precision introduces quantisation error. If a weight distribution has many outliers, compressing it to 4 bits can distort the range, leading to degraded fluency or higher perplexity. Accuracy loss depends on the scheme used and the model architecture.</p><p>Some approaches leave sensitive layers in higher precision (embeddings, normalisation layers) while quantising others. Others combine quantisation with retraining so the model learns to be robust.</p><hr><h2 id=outlook>Outlook<a hidden class=anchor aria-hidden=true href=#outlook>#</a></h2><p>Quantisation is not the only approach — pruning and distillation are also active research areas — but it is the most direct route to shrinking a model while keeping the same architecture. In the next post, I’ll break down how quantisation works in practice, the different schemes used, and the tools available today.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=https://rossn1.github.io/posts/how-quantisation-works/><span class=title>Next »</span><br><span>How Quantisation Works: From FP32 to INT4</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://rossn1.github.io/>ross.</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>