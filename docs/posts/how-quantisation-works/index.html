<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>How Quantisation Works: From FP32 to INT4 | ross.</title><meta name=keywords content><meta name=description content="In the first post I explained why quantisation matters. Here I’ll focus on how it is applied, the main schemes in use, and some of the libraries that make it practical.

Precision levels

FP32: full precision, used in training. High accuracy, but memory-intensive.
FP16 / bfloat16: half precision. Often the default for training and inference on GPUs.
INT8: 8-bit integers. Reduces memory by 4× compared to FP32.
INT4: 4-bit integers. More aggressive, but often still usable for inference.

Each step down saves memory and bandwidth but increases quantisation error."><meta name=author content><link rel=canonical href=https://rossn1.github.io/posts/how-quantisation-works/><link crossorigin=anonymous href=https://rossn1.github.io/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://rossn1.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://rossn1.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://rossn1.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://rossn1.github.io/apple-touch-icon.png><link rel=mask-icon href=https://rossn1.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://rossn1.github.io/posts/how-quantisation-works/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://rossn1.github.io/posts/how-quantisation-works/"><meta property="og:site_name" content="ross."><meta property="og:title" content="How Quantisation Works: From FP32 to INT4"><meta property="og:description" content="In the first post I explained why quantisation matters. Here I’ll focus on how it is applied, the main schemes in use, and some of the libraries that make it practical.
Precision levels FP32: full precision, used in training. High accuracy, but memory-intensive. FP16 / bfloat16: half precision. Often the default for training and inference on GPUs. INT8: 8-bit integers. Reduces memory by 4× compared to FP32. INT4: 4-bit integers. More aggressive, but often still usable for inference. Each step down saves memory and bandwidth but increases quantisation error."><meta property="og:locale" content="en-gb"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-09-24T10:00:00+01:00"><meta property="article:modified_time" content="2025-09-24T10:00:00+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="How Quantisation Works: From FP32 to INT4"><meta name=twitter:description content="In the first post I explained why quantisation matters. Here I’ll focus on how it is applied, the main schemes in use, and some of the libraries that make it practical.

Precision levels

FP32: full precision, used in training. High accuracy, but memory-intensive.
FP16 / bfloat16: half precision. Often the default for training and inference on GPUs.
INT8: 8-bit integers. Reduces memory by 4× compared to FP32.
INT4: 4-bit integers. More aggressive, but often still usable for inference.

Each step down saves memory and bandwidth but increases quantisation error."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://rossn1.github.io/posts/"},{"@type":"ListItem","position":2,"name":"How Quantisation Works: From FP32 to INT4","item":"https://rossn1.github.io/posts/how-quantisation-works/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"How Quantisation Works: From FP32 to INT4","name":"How Quantisation Works: From FP32 to INT4","description":"In the first post I explained why quantisation matters. Here I’ll focus on how it is applied, the main schemes in use, and some of the libraries that make it practical.\nPrecision levels FP32: full precision, used in training. High accuracy, but memory-intensive. FP16 / bfloat16: half precision. Often the default for training and inference on GPUs. INT8: 8-bit integers. Reduces memory by 4× compared to FP32. INT4: 4-bit integers. More aggressive, but often still usable for inference. Each step down saves memory and bandwidth but increases quantisation error.\n","keywords":[],"articleBody":"In the first post I explained why quantisation matters. Here I’ll focus on how it is applied, the main schemes in use, and some of the libraries that make it practical.\nPrecision levels FP32: full precision, used in training. High accuracy, but memory-intensive. FP16 / bfloat16: half precision. Often the default for training and inference on GPUs. INT8: 8-bit integers. Reduces memory by 4× compared to FP32. INT4: 4-bit integers. More aggressive, but often still usable for inference. Each step down saves memory and bandwidth but increases quantisation error.\nMapping values to integers A floating point value x is quantised using:\nq = round(x / scale) + zero_point scale: defines how much of the float range is covered by the integer range. zero_point: shifts the mapping so zero can be represented exactly. To recover the value:\nx_approx = (q - zero_point) * scale This means quantisation is essentially a rounding operation.\nSymmetric vs asymmetric Symmetric: centred around zero, same range for positive and negative values. Efficient, but not good for skewed distributions. Asymmetric: introduces a zero point so the integer range can shift. Better for weights/activations that aren’t balanced around zero. PTQ vs QAT Post-Training Quantisation (PTQ): apply quantisation after training. It’s quick and works without retraining, but may lose accuracy. Quantisation-Aware Training (QAT): simulate quantisation during training so the model adapts. More costly, but usually better accuracy. Techniques for reducing loss Several methods go beyond naive quantisation:\nSmoothQuant: shifts outlier activations into weights to make ranges easier to quantise. SpQR: leaves outliers at higher precision and quantises the rest to 3–4 bits. SpinQuant: rotates weight subspaces before quantisation, reducing distortion. These are research methods, but they demonstrate how much engineering goes into retaining accuracy at low bit widths.\nLibraries and frameworks bitsandbytes: integrates with Hugging Face Transformers. Supports 8-bit and 4-bit loading, and QLoRA fine-tuning. GPTQ: a post-training method tailored for LLMs, often used for 4-bit quantisation. AWQ: activation-aware weight quantisation, another PTQ method. llama.cpp / GGUF: CPU-friendly inference, supporting multiple quantisation levels, widely used for edge deployment. Practical notes Not all layers need quantising. Embeddings and normalisation often stay in FP16. Calibration with representative input data improves PTQ accuracy. Hardware support matters: some CPUs/NPUs handle INT8 well, but INT4 kernels are less standard. In the next post I’ll show what this looks like in practice: using bitsandbytes to load a model in 4-bit precision on a GPU, and converting a model to GGUF for inference with llama.cpp on a CPU.\n","wordCount":"412","inLanguage":"en","datePublished":"2025-09-24T10:00:00+01:00","dateModified":"2025-09-24T10:00:00+01:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://rossn1.github.io/posts/how-quantisation-works/"},"publisher":{"@type":"Organization","name":"ross.","logo":{"@type":"ImageObject","url":"https://rossn1.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://rossn1.github.io/ accesskey=h title="ross. (Alt + H)">ross.</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://rossn1.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://rossn1.github.io/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">How Quantisation Works: From FP32 to INT4</h1><div class=post-meta><span title='2025-09-24 10:00:00 +0100 BST'>September 24, 2025</span>&nbsp;·&nbsp;2 min</div></header><div class=post-content><p>In the first post I explained why quantisation matters. Here I’ll focus on how it is applied, the main schemes in use, and some of the libraries that make it practical.</p><hr><h2 id=precision-levels>Precision levels<a hidden class=anchor aria-hidden=true href=#precision-levels>#</a></h2><ul><li><strong>FP32</strong>: full precision, used in training. High accuracy, but memory-intensive.</li><li><strong>FP16 / bfloat16</strong>: half precision. Often the default for training and inference on GPUs.</li><li><strong>INT8</strong>: 8-bit integers. Reduces memory by 4× compared to FP32.</li><li><strong>INT4</strong>: 4-bit integers. More aggressive, but often still usable for inference.</li></ul><p>Each step down saves memory and bandwidth but increases quantisation error.</p><hr><h2 id=mapping-values-to-integers>Mapping values to integers<a hidden class=anchor aria-hidden=true href=#mapping-values-to-integers>#</a></h2><p>A floating point value <em>x</em> is quantised using:</p><pre tabindex=0><code>
q = round(x / scale) + zero_point
</code></pre><ul><li><strong>scale</strong>: defines how much of the float range is covered by the integer range.</li><li><strong>zero_point</strong>: shifts the mapping so zero can be represented exactly.</li></ul><p>To recover the value:</p><pre tabindex=0><code>
x_approx = (q - zero_point) * scale
</code></pre><p>This means quantisation is essentially a rounding operation.</p><hr><h2 id=symmetric-vs-asymmetric>Symmetric vs asymmetric<a hidden class=anchor aria-hidden=true href=#symmetric-vs-asymmetric>#</a></h2><ul><li><strong>Symmetric</strong>: centred around zero, same range for positive and negative values. Efficient, but not good for skewed distributions.</li><li><strong>Asymmetric</strong>: introduces a zero point so the integer range can shift. Better for weights/activations that aren’t balanced around zero.</li></ul><hr><h2 id=ptq-vs-qat>PTQ vs QAT<a hidden class=anchor aria-hidden=true href=#ptq-vs-qat>#</a></h2><ul><li><strong>Post-Training Quantisation (PTQ)</strong>: apply quantisation after training. It’s quick and works without retraining, but may lose accuracy.</li><li><strong>Quantisation-Aware Training (QAT)</strong>: simulate quantisation during training so the model adapts. More costly, but usually better accuracy.</li></ul><hr><h2 id=techniques-for-reducing-loss>Techniques for reducing loss<a hidden class=anchor aria-hidden=true href=#techniques-for-reducing-loss>#</a></h2><p>Several methods go beyond naive quantisation:</p><ul><li><strong>SmoothQuant</strong>: shifts outlier activations into weights to make ranges easier to quantise.</li><li><strong>SpQR</strong>: leaves outliers at higher precision and quantises the rest to 3–4 bits.</li><li><strong>SpinQuant</strong>: rotates weight subspaces before quantisation, reducing distortion.</li></ul><p>These are research methods, but they demonstrate how much engineering goes into retaining accuracy at low bit widths.</p><hr><h2 id=libraries-and-frameworks>Libraries and frameworks<a hidden class=anchor aria-hidden=true href=#libraries-and-frameworks>#</a></h2><ul><li><strong>bitsandbytes</strong>: integrates with Hugging Face Transformers. Supports 8-bit and 4-bit loading, and QLoRA fine-tuning.</li><li><strong>GPTQ</strong>: a post-training method tailored for LLMs, often used for 4-bit quantisation.</li><li><strong>AWQ</strong>: activation-aware weight quantisation, another PTQ method.</li><li><strong>llama.cpp / GGUF</strong>: CPU-friendly inference, supporting multiple quantisation levels, widely used for edge deployment.</li></ul><hr><h2 id=practical-notes>Practical notes<a hidden class=anchor aria-hidden=true href=#practical-notes>#</a></h2><ul><li>Not all layers need quantising. Embeddings and normalisation often stay in FP16.</li><li>Calibration with representative input data improves PTQ accuracy.</li><li>Hardware support matters: some CPUs/NPUs handle INT8 well, but INT4 kernels are less standard.</li></ul><hr><p>In the next post I’ll show what this looks like in practice: using <code>bitsandbytes</code> to load a model in 4-bit precision on a GPU, and converting a model to GGUF for inference with <code>llama.cpp</code> on a CPU.</p><hr></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://rossn1.github.io/posts/why-quantisation-matters-for-llms/><span class=title>« Prev</span><br><span>Why Quantisation Matters for LLMs</span>
</a><a class=next href=https://rossn1.github.io/posts/deploying-quantised-llms/><span class=title>Next »</span><br><span>Deploying Quantised LLMs</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://rossn1.github.io/>ross.</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>