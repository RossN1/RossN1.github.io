<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Mixed precision training in large language models | ross.</title>
<meta name="keywords" content="">
<meta name="description" content="Mixed precision training in large language models
Neural networks have traditionally been trained using 32-bit floating point (FP32). However, the precision FP32 offers isn&rsquo;t always necessary for most training workloads. Lower precision formats can be just as effective, as training is inherently approximate.
This post covers how I added mixed precision training to an implementation of the transformer architecture following Umar Jamil&rsquo;s tutorial and what I learned along the way.
What is mixed precision training?
Mixed precision training is an optimization technique that utilises 16-bit (half precision) for most operations and 32-bit (full precision) for numerically sensitive operations, increasing the speed of model training and reducing memory usage.">
<meta name="author" content="">
<link rel="canonical" href="https://rossn1.github.io/posts/mixed-precision-training-in-large-language-models/">
<link crossorigin="anonymous" href="https://rossn1.github.io/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<link rel="icon" href="https://rossn1.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://rossn1.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://rossn1.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://rossn1.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://rossn1.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://rossn1.github.io/posts/mixed-precision-training-in-large-language-models/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="https://rossn1.github.io/posts/mixed-precision-training-in-large-language-models/">
  <meta property="og:site_name" content="ross.">
  <meta property="og:title" content="Mixed precision training in large language models">
  <meta property="og:description" content="Mixed precision training in large language models Neural networks have traditionally been trained using 32-bit floating point (FP32). However, the precision FP32 offers isn’t always necessary for most training workloads. Lower precision formats can be just as effective, as training is inherently approximate.
This post covers how I added mixed precision training to an implementation of the transformer architecture following Umar Jamil’s tutorial and what I learned along the way.
What is mixed precision training? Mixed precision training is an optimization technique that utilises 16-bit (half precision) for most operations and 32-bit (full precision) for numerically sensitive operations, increasing the speed of model training and reducing memory usage.">
  <meta property="og:locale" content="en-gb">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-12-16T15:40:00+00:00">
    <meta property="article:modified_time" content="2025-12-16T15:40:00+00:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Mixed precision training in large language models">
<meta name="twitter:description" content="Mixed precision training in large language models
Neural networks have traditionally been trained using 32-bit floating point (FP32). However, the precision FP32 offers isn&rsquo;t always necessary for most training workloads. Lower precision formats can be just as effective, as training is inherently approximate.
This post covers how I added mixed precision training to an implementation of the transformer architecture following Umar Jamil&rsquo;s tutorial and what I learned along the way.
What is mixed precision training?
Mixed precision training is an optimization technique that utilises 16-bit (half precision) for most operations and 32-bit (full precision) for numerically sensitive operations, increasing the speed of model training and reducing memory usage.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://rossn1.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Mixed precision training in large language models",
      "item": "https://rossn1.github.io/posts/mixed-precision-training-in-large-language-models/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Mixed precision training in large language models",
  "name": "Mixed precision training in large language models",
  "description": "Mixed precision training in large language models Neural networks have traditionally been trained using 32-bit floating point (FP32). However, the precision FP32 offers isn\u0026rsquo;t always necessary for most training workloads. Lower precision formats can be just as effective, as training is inherently approximate.\nThis post covers how I added mixed precision training to an implementation of the transformer architecture following Umar Jamil\u0026rsquo;s tutorial and what I learned along the way.\nWhat is mixed precision training? Mixed precision training is an optimization technique that utilises 16-bit (half precision) for most operations and 32-bit (full precision) for numerically sensitive operations, increasing the speed of model training and reducing memory usage.\n",
  "keywords": [
    
  ],
  "articleBody": "Mixed precision training in large language models Neural networks have traditionally been trained using 32-bit floating point (FP32). However, the precision FP32 offers isn’t always necessary for most training workloads. Lower precision formats can be just as effective, as training is inherently approximate.\nThis post covers how I added mixed precision training to an implementation of the transformer architecture following Umar Jamil’s tutorial and what I learned along the way.\nWhat is mixed precision training? Mixed precision training is an optimization technique that utilises 16-bit (half precision) for most operations and 32-bit (full precision) for numerically sensitive operations, increasing the speed of model training and reducing memory usage.\nMixed precision typically involves three steps\nCasting By casting model operations such as convolutions, matrix multiplication and linear layers to half-precision, this would reduce the required memory bandwidth and increase the speed of calculation\nMaintaining FP32 weights Initially, a copy of the model’s parameters is saved in FP32. During the backpropagation stage of the training process, gradients would be computed in FP16 and applied to the FP32 master weights, preventing small gradient updates from being lost due to the limited range of FP16, preventing gradient underflow.\nLoss Scaling The value of the loss function is multiplied by a scaling factor before backpropagation. This would shift the values of the gradients into a range that can be represented by FP16, avoiding underflow errors. After backpropagation, the gradients would then be divided by the same factor, before updating weights in FP32. If the scaling factor is too large, gradients can overflow (become infinity/NaN). In practice, dynamic loss scaling would be used. If overflow is detected, the training step would be skipped and the scale factor would be reduced.\nImplementing mixed precision training in PyTorch PyTorch provides two functions for implementing mixed precision training: autocast and GradScaler.\nAutocast wraps the forward pass and selects FP16 or FP32 for each operation. Matrix multiplication and convolutions run in FP16, numerically sensitive operations (Such as loss calculations) stay in FP32.\nGradScaler handles loss scaling. It is used for scaling the loss before backpropagation, checks for overflow and unscales the gradients before the optimizer step\nfrom torch.amp import autocast, GradScaler scaler = GradScaler('cuda') for epoch in range(initial_epoch, config['num_epochs']): batch_iterator = tqdm(train_dataloader, desc=f\"Processing epoch {epoch:02d}\") for batch in batch_iterator: model.train() optimizer.zero_grad() encoder_input = batch['encoder_input'].to(device) decoder_input = batch['decoder_input'].to(device) encoder_mask = batch['encoder_mask'].to(device) decoder_mask = batch['decoder_mask'].to(device) with autocast('cuda'): encoder_output = model.encode(encoder_input, encoder_mask) decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) proj_output = model.project(decoder_output) label = batch['label'].to(device) loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1)) scaler.scale(loss).backward() scaler.step(optimizer) scaler.update() The changes from the standard model training process are:\nautocast('cuda') - This wraps the forward pass and loss calculation. PyTorch would use FP16 where it is safe (matrix multiplication, convolutions), and keep FP32 where it is needed (Softmax, loss functions, layer normalisation) scaler.scale(loss).backward() - Multiplies the loss by the scale factor before backpropagation. This shifts gradients into a range that can be used by FP16. scaler.step(optimizer) - Unscales the gradients back to the original values, checks for overflow, and updates the weights. If overflow is detected, the step is skipped. scaler.update - Adjusts the scale factor for the next iteration. If overflow occurred, the scale is reduced. If several iterations pass without overflow, the scale is increased to maximise the benefits of scaling. Conclusion Mixed precision training offers a practical way to accelerate deep learning workloads with minimal code changes. By wrapping the forward pass in autocast and using GradScaler to handle gradient scaling, I achieved a ~49% increase in throughput and reduced training time by 33% for a single epoch.\nFP32 (Baseline):\nProcessing Epoch 00: 100%|██████████| 14297/14297 [1:49:58\u003c00:00, 2.17it/s, loss=4.376] Mixed Precision:\nProcessing Epoch 00: 100%|██████████| 14297/14297 [1:13:46\u003c00:00, 3.23it/s, loss=4.733] While the mixed precision run showed a slightly higher loss after one epoch (4.733 vs 4.376), this is expected behaviour — the two approaches typically converge to similar values over additional epochs.\nThe key insight is that mixed precision doesn’t sacrifice model quality; it simply gets you there faster.\n",
  "wordCount" : "659",
  "inLanguage": "en",
  "datePublished": "2025-12-16T15:40:00Z",
  "dateModified": "2025-12-16T15:40:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://rossn1.github.io/posts/mixed-precision-training-in-large-language-models/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "ross.",
    "logo": {
      "@type": "ImageObject",
      "url": "https://rossn1.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://rossn1.github.io/" accesskey="h" title="ross. (Alt + H)">ross.</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://rossn1.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://rossn1.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Mixed precision training in large language models
    </h1>
    <div class="post-meta"><span title='2025-12-16 15:40:00 +0000 GMT'>December 16, 2025</span>&nbsp;·&nbsp;4 min

</div>
  </header> 
  <div class="post-content"><h1 id="mixed-precision-training-in-large-language-models">Mixed precision training in large language models<a hidden class="anchor" aria-hidden="true" href="#mixed-precision-training-in-large-language-models">#</a></h1>
<p>Neural networks have traditionally been trained using 32-bit floating point (FP32). However, the precision FP32 offers isn&rsquo;t always necessary for most training workloads. Lower precision formats can be just as effective, as training is inherently approximate.</p>
<p>This post covers how I added mixed precision training to an implementation of the transformer architecture following <a href="https://www.youtube.com/watch?v=ISNdQcPhsts">Umar Jamil&rsquo;s tutorial</a> and what I learned along the way.</p>
<h2 id="what-is-mixed-precision-training">What is mixed precision training?<a hidden class="anchor" aria-hidden="true" href="#what-is-mixed-precision-training">#</a></h2>
<p>Mixed precision training is an optimization technique that utilises 16-bit (half precision) for most operations and 32-bit (full precision) for numerically sensitive operations, increasing the speed of model training and reducing memory usage.</p>
<p>Mixed precision typically involves three steps</p>
<h3 id="casting">Casting<a hidden class="anchor" aria-hidden="true" href="#casting">#</a></h3>
<p>By casting model operations such as convolutions, matrix multiplication and linear layers to half-precision, this would reduce the required memory bandwidth and increase the speed of calculation</p>
<h3 id="maintaining-fp32-weights">Maintaining FP32 weights<a hidden class="anchor" aria-hidden="true" href="#maintaining-fp32-weights">#</a></h3>
<p>Initially, a copy of the model&rsquo;s parameters is saved in FP32. During the backpropagation stage of the training process, gradients would be computed in FP16 and applied to the FP32 master weights, preventing small gradient updates from being lost due to the limited range of FP16, preventing gradient underflow.</p>
<h3 id="loss-scaling">Loss Scaling<a hidden class="anchor" aria-hidden="true" href="#loss-scaling">#</a></h3>
<p>The value of the loss function is multiplied by a scaling factor before backpropagation. This would shift the values of the gradients into a range that can be represented by FP16, avoiding underflow errors. After backpropagation, the gradients would then be divided by the same factor, before updating weights in FP32.
<!-- raw HTML omitted --><!-- raw HTML omitted -->If the scaling factor is too large, gradients can overflow (become infinity/NaN). In practice, dynamic loss scaling would be used. If overflow is detected, the training step would be skipped and the scale factor would be reduced.</p>
<h2 id="implementing-mixed-precision-training-in-pytorch">Implementing mixed precision training in PyTorch<a hidden class="anchor" aria-hidden="true" href="#implementing-mixed-precision-training-in-pytorch">#</a></h2>
<p>PyTorch provides two functions for implementing mixed precision training: <code>autocast</code> and <code>GradScaler</code>.</p>
<p>Autocast wraps the forward pass and selects FP16 or FP32 for each operation. Matrix multiplication and convolutions run in FP16, numerically sensitive operations (Such as loss calculations) stay in FP32.</p>
<p>GradScaler handles loss scaling. It is used for scaling the loss before backpropagation, checks for overflow and unscales the gradients before the optimizer step</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> torch.amp <span style="color:#f92672">import</span> autocast, GradScaler
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>scaler <span style="color:#f92672">=</span> GradScaler(<span style="color:#e6db74">&#39;cuda&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(initial_epoch, config[<span style="color:#e6db74">&#39;num_epochs&#39;</span>]):
</span></span><span style="display:flex;"><span>    batch_iterator <span style="color:#f92672">=</span> tqdm(train_dataloader, desc<span style="color:#f92672">=</span><span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Processing epoch </span><span style="color:#e6db74">{</span>epoch<span style="color:#e6db74">:</span><span style="color:#e6db74">02d</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> batch <span style="color:#f92672">in</span> batch_iterator:
</span></span><span style="display:flex;"><span>        model<span style="color:#f92672">.</span>train()
</span></span><span style="display:flex;"><span>        optimizer<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        encoder_input <span style="color:#f92672">=</span> batch[<span style="color:#e6db74">&#39;encoder_input&#39;</span>]<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>        decoder_input <span style="color:#f92672">=</span> batch[<span style="color:#e6db74">&#39;decoder_input&#39;</span>]<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>        encoder_mask <span style="color:#f92672">=</span> batch[<span style="color:#e6db74">&#39;encoder_mask&#39;</span>]<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>        decoder_mask <span style="color:#f92672">=</span> batch[<span style="color:#e6db74">&#39;decoder_mask&#39;</span>]<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> autocast(<span style="color:#e6db74">&#39;cuda&#39;</span>):
</span></span><span style="display:flex;"><span>            encoder_output <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>encode(encoder_input, encoder_mask)
</span></span><span style="display:flex;"><span>            decoder_output <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>decode(encoder_output, encoder_mask, decoder_input, decoder_mask)
</span></span><span style="display:flex;"><span>            proj_output <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>project(decoder_output)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            label <span style="color:#f92672">=</span> batch[<span style="color:#e6db74">&#39;label&#39;</span>]<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>            loss <span style="color:#f92672">=</span> loss_fn(proj_output<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, tokenizer_tgt<span style="color:#f92672">.</span>get_vocab_size()), label<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        scaler<span style="color:#f92672">.</span>scale(loss)<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>        scaler<span style="color:#f92672">.</span>step(optimizer)
</span></span><span style="display:flex;"><span>        scaler<span style="color:#f92672">.</span>update()
</span></span></code></pre></div><p>The changes from the standard model training process are:</p>
<ul>
<li><code>autocast('cuda')</code> - This wraps the forward pass and loss calculation. PyTorch would use FP16 where it is safe (matrix multiplication, convolutions), and keep FP32 where it is needed (Softmax, loss functions, layer normalisation)</li>
<li><code>scaler.scale(loss).backward()</code> - Multiplies the loss by the scale factor before backpropagation. This shifts gradients into a range that can be used by FP16.</li>
<li><code>scaler.step(optimizer)</code> - Unscales the gradients back to the original values, checks for overflow, and updates the weights. If overflow is detected, the step is skipped.</li>
<li><code>scaler.update</code> - Adjusts the scale factor for the next iteration. If overflow occurred, the scale is reduced. If several iterations pass without overflow, the scale is increased to maximise the benefits of scaling.</li>
</ul>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>Mixed precision training offers a practical way to accelerate deep learning workloads with minimal code changes. By wrapping the forward pass in <code>autocast</code> and using <code>GradScaler</code> to handle gradient scaling, I achieved a <strong>~49%</strong> increase in throughput and reduced training time by <strong>33%</strong> for a single epoch.</p>
<p><strong>FP32 (Baseline):</strong></p>
<pre tabindex="0"><code>Processing Epoch 00: 100%|██████████| 14297/14297 [1:49:58&lt;00:00, 2.17it/s, loss=4.376]
</code></pre><p><strong>Mixed Precision:</strong></p>
<pre tabindex="0"><code>Processing Epoch 00: 100%|██████████| 14297/14297 [1:13:46&lt;00:00, 3.23it/s, loss=4.733]
</code></pre><p>While the mixed precision run showed a slightly higher loss after one epoch (4.733 vs 4.376), this is expected behaviour — the two approaches typically converge to similar values over additional epochs.</p>
<p>The key insight is that mixed precision doesn&rsquo;t sacrifice model quality; it simply gets you there faster.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://rossn1.github.io/">ross.</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
