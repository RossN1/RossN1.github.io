<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Deploying Quantised LLMs | ross.</title><meta name=keywords content><meta name=description content="In the first two posts, I explained why quantisation matters and how it works. Here I’ll cover how to deploy quantised large language models (LLMs) on edge hardware, what workflows are available, and what to measure in practice.

Deployment workflows
1. Hugging Face Transformers + bitsandbytes (GPU or desktop)
This is the most common path if you have a consumer GPU. bitsandbytes integrates with Transformers to load models directly in 8-bit or 4-bit precision."><meta name=author content><link rel=canonical href=https://rossn1.github.io/posts/deploying-quantised-llms/><link crossorigin=anonymous href=https://rossn1.github.io/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://rossn1.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://rossn1.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://rossn1.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://rossn1.github.io/apple-touch-icon.png><link rel=mask-icon href=https://rossn1.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://rossn1.github.io/posts/deploying-quantised-llms/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://rossn1.github.io/posts/deploying-quantised-llms/"><meta property="og:site_name" content="ross."><meta property="og:title" content="Deploying Quantised LLMs"><meta property="og:description" content="In the first two posts, I explained why quantisation matters and how it works. Here I’ll cover how to deploy quantised large language models (LLMs) on edge hardware, what workflows are available, and what to measure in practice.
Deployment workflows 1. Hugging Face Transformers + bitsandbytes (GPU or desktop) This is the most common path if you have a consumer GPU. bitsandbytes integrates with Transformers to load models directly in 8-bit or 4-bit precision."><meta property="og:locale" content="en-gb"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-09-24T09:00:00+01:00"><meta property="article:modified_time" content="2025-09-24T09:00:00+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Deploying Quantised LLMs"><meta name=twitter:description content="In the first two posts, I explained why quantisation matters and how it works. Here I’ll cover how to deploy quantised large language models (LLMs) on edge hardware, what workflows are available, and what to measure in practice.

Deployment workflows
1. Hugging Face Transformers + bitsandbytes (GPU or desktop)
This is the most common path if you have a consumer GPU. bitsandbytes integrates with Transformers to load models directly in 8-bit or 4-bit precision."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://rossn1.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Deploying Quantised LLMs","item":"https://rossn1.github.io/posts/deploying-quantised-llms/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Deploying Quantised LLMs","name":"Deploying Quantised LLMs","description":"In the first two posts, I explained why quantisation matters and how it works. Here I’ll cover how to deploy quantised large language models (LLMs) on edge hardware, what workflows are available, and what to measure in practice.\nDeployment workflows 1. Hugging Face Transformers + bitsandbytes (GPU or desktop) This is the most common path if you have a consumer GPU. bitsandbytes integrates with Transformers to load models directly in 8-bit or 4-bit precision.\n","keywords":[],"articleBody":"In the first two posts, I explained why quantisation matters and how it works. Here I’ll cover how to deploy quantised large language models (LLMs) on edge hardware, what workflows are available, and what to measure in practice.\nDeployment workflows 1. Hugging Face Transformers + bitsandbytes (GPU or desktop) This is the most common path if you have a consumer GPU. bitsandbytes integrates with Transformers to load models directly in 8-bit or 4-bit precision.\nExample:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig model_name = \"meta-llama/Llama-2-7b-chat-hf\" tokenizer = AutoTokenizer.from_pretrained(model_name) bnb_config = BitsAndBytesConfig(load_in_4bit=True) model = AutoModelForCausalLM.from_pretrained( model_name, quantization_config=bnb_config, device_map=\"auto\" ) inputs = tokenizer(\"Hello world\", return_tensors=\"pt\").input_ids outputs = model.generate(inputs, max_new_tokens=50) print(tokenizer.decode(outputs[0])) This loads the 7B model in 4-bit precision. On a consumer GPU with 12–16 GB VRAM, that can be the difference between failing to load and running smoothly.\n2. llama.cpp + GGUF (CPU or edge devices) For devices without a GPU, llama.cpp is a lightweight inference framework written in C++. It runs quantised models on CPU and supports Apple Silicon, ARM boards, and even mobile phones.\nSteps:\nConvert the model Convert a Hugging Face checkpoint into GGUF format with chosen quantisation (e.g. Q4).\npython convert-hf-to-gguf.py --model llama-2-7b --out llama-2-7b-q4.gguf Run inference\n./llama.cpp/llama-cli -m llama-2-7b-q4.gguf -p \"Explain quantisation in simple terms\" The GGUF format supports a range of quantisation types (Q2, Q4, Q8). Lower-bit variants reduce memory further but can affect quality.\n3. ONNX Runtime / TensorRT (accelerated inference) For deployment on embedded GPUs or NPUs, ONNX Runtime and TensorRT both support quantised models. These are more common in production pipelines where integration with existing inference stacks is needed.\nWhat to measure When deploying quantised models, I focus on four metrics:\nLatency: tokens per second generated. Memory usage: peak RAM/VRAM during inference. Output quality: usually measured by perplexity or comparison against benchmarks. Power consumption: relevant on mobile or embedded hardware. A practical example:\nA 7B model in FP16 may run at ~10 tokens/sec on a laptop GPU and consume ~14 GB VRAM. The same model in 4-bit quantisation can drop to ~3.5 GB VRAM and ~20 tokens/sec, with only a small drop in fluency. Trade-offs and pitfalls Accuracy loss: INT8 is usually safe; INT4 sometimes reduces fluency or coherence. Hybrid precision (e.g. embeddings in FP16, attention weights in INT4) can help. Hardware support: INT8 kernels are common, INT4 less so. llama.cpp handles this internally, but other runtimes may not. Calibration: PTQ benefits from representative input data to compute scales. Poor calibration increases error. Security risks: research has shown that quantisation can hide adversarial triggers in weights. While not a barrier to use, it’s a factor for production systems. Future directions Research is pushing quantisation further:\nMixed precision: combining FP16, INT8, and INT4 within a model. Outlier handling: methods like SpQR preserve rare but important weights in higher precision. Rotation-based schemes: SpinQuant aligns weight spaces for better quantisation. Integration with pruning and distillation: combining techniques to maximise efficiency. Hardware support: new accelerators are emerging with native INT4 instructions. Closing thoughts Quantisation is a trade-off: you exchange some model quality for a smaller footprint and faster execution. For edge deployment, it often makes the difference between “impossible” and “usable”. With careful choice of bit width, calibration, and tools, it’s possible to run models that once needed datacentres directly on laptops or embedded devices.\n","wordCount":"547","inLanguage":"en","datePublished":"2025-09-24T09:00:00+01:00","dateModified":"2025-09-24T09:00:00+01:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://rossn1.github.io/posts/deploying-quantised-llms/"},"publisher":{"@type":"Organization","name":"ross.","logo":{"@type":"ImageObject","url":"https://rossn1.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://rossn1.github.io/ accesskey=h title="ross. (Alt + H)">ross.</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://rossn1.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://rossn1.github.io/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Deploying Quantised LLMs</h1><div class=post-meta><span title='2025-09-24 09:00:00 +0100 BST'>September 24, 2025</span>&nbsp;·&nbsp;3 min</div></header><div class=post-content><p>In the first two posts, I explained why quantisation matters and how it works. Here I’ll cover how to deploy quantised large language models (LLMs) on edge hardware, what workflows are available, and what to measure in practice.</p><hr><h2 id=deployment-workflows>Deployment workflows<a hidden class=anchor aria-hidden=true href=#deployment-workflows>#</a></h2><h3 id=1-hugging-face-transformers--bitsandbytes-gpu-or-desktop>1. Hugging Face Transformers + bitsandbytes (GPU or desktop)<a hidden class=anchor aria-hidden=true href=#1-hugging-face-transformers--bitsandbytes-gpu-or-desktop>#</a></h3><p>This is the most common path if you have a consumer GPU. <code>bitsandbytes</code> integrates with Transformers to load models directly in 8-bit or 4-bit precision.</p><p>Example:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model_name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;meta-llama/Llama-2-7b-chat-hf&#34;</span>
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(model_name)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>bnb_config <span style=color:#f92672>=</span> BitsAndBytesConfig(load_in_4bit<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> AutoModelForCausalLM<span style=color:#f92672>.</span>from_pretrained(
</span></span><span style=display:flex><span>    model_name,
</span></span><span style=display:flex><span>    quantization_config<span style=color:#f92672>=</span>bnb_config,
</span></span><span style=display:flex><span>    device_map<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;auto&#34;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>inputs <span style=color:#f92672>=</span> tokenizer(<span style=color:#e6db74>&#34;Hello world&#34;</span>, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;pt&#34;</span>)<span style=color:#f92672>.</span>input_ids
</span></span><span style=display:flex><span>outputs <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>generate(inputs, max_new_tokens<span style=color:#f92672>=</span><span style=color:#ae81ff>50</span>)
</span></span><span style=display:flex><span>print(tokenizer<span style=color:#f92672>.</span>decode(outputs[<span style=color:#ae81ff>0</span>]))
</span></span></code></pre></div><p>This loads the 7B model in 4-bit precision. On a consumer GPU with 12–16 GB VRAM, that can be the difference between failing to load and running smoothly.</p><hr><h3 id=2-llamacpp--gguf-cpu-or-edge-devices>2. llama.cpp + GGUF (CPU or edge devices)<a hidden class=anchor aria-hidden=true href=#2-llamacpp--gguf-cpu-or-edge-devices>#</a></h3><p>For devices without a GPU, <code>llama.cpp</code> is a lightweight inference framework written in C++. It runs quantised models on CPU and supports Apple Silicon, ARM boards, and even mobile phones.</p><p>Steps:</p><ol><li><p><strong>Convert the model</strong>
Convert a Hugging Face checkpoint into GGUF format with chosen quantisation (e.g. Q4).</p><pre tabindex=0><code>python convert-hf-to-gguf.py --model llama-2-7b --out llama-2-7b-q4.gguf
</code></pre></li><li><p><strong>Run inference</strong></p><pre tabindex=0><code>./llama.cpp/llama-cli -m llama-2-7b-q4.gguf -p &#34;Explain quantisation in simple terms&#34;
</code></pre></li></ol><p>The GGUF format supports a range of quantisation types (Q2, Q4, Q8). Lower-bit variants reduce memory further but can affect quality.</p><hr><h3 id=3-onnx-runtime--tensorrt-accelerated-inference>3. ONNX Runtime / TensorRT (accelerated inference)<a hidden class=anchor aria-hidden=true href=#3-onnx-runtime--tensorrt-accelerated-inference>#</a></h3><p>For deployment on embedded GPUs or NPUs, ONNX Runtime and TensorRT both support quantised models. These are more common in production pipelines where integration with existing inference stacks is needed.</p><hr><h2 id=what-to-measure>What to measure<a hidden class=anchor aria-hidden=true href=#what-to-measure>#</a></h2><p>When deploying quantised models, I focus on four metrics:</p><ol><li><strong>Latency</strong>: tokens per second generated.</li><li><strong>Memory usage</strong>: peak RAM/VRAM during inference.</li><li><strong>Output quality</strong>: usually measured by perplexity or comparison against benchmarks.</li><li><strong>Power consumption</strong>: relevant on mobile or embedded hardware.</li></ol><p>A practical example:</p><ul><li>A 7B model in FP16 may run at ~10 tokens/sec on a laptop GPU and consume ~14 GB VRAM.</li><li>The same model in 4-bit quantisation can drop to ~3.5 GB VRAM and ~20 tokens/sec, with only a small drop in fluency.</li></ul><hr><h2 id=trade-offs-and-pitfalls>Trade-offs and pitfalls<a hidden class=anchor aria-hidden=true href=#trade-offs-and-pitfalls>#</a></h2><ul><li><strong>Accuracy loss</strong>: INT8 is usually safe; INT4 sometimes reduces fluency or coherence. Hybrid precision (e.g. embeddings in FP16, attention weights in INT4) can help.</li><li><strong>Hardware support</strong>: INT8 kernels are common, INT4 less so. llama.cpp handles this internally, but other runtimes may not.</li><li><strong>Calibration</strong>: PTQ benefits from representative input data to compute scales. Poor calibration increases error.</li><li><strong>Security risks</strong>: research has shown that quantisation can hide adversarial triggers in weights. While not a barrier to use, it’s a factor for production systems.</li></ul><hr><h2 id=future-directions>Future directions<a hidden class=anchor aria-hidden=true href=#future-directions>#</a></h2><p>Research is pushing quantisation further:</p><ul><li><strong>Mixed precision</strong>: combining FP16, INT8, and INT4 within a model.</li><li><strong>Outlier handling</strong>: methods like SpQR preserve rare but important weights in higher precision.</li><li><strong>Rotation-based schemes</strong>: SpinQuant aligns weight spaces for better quantisation.</li><li><strong>Integration with pruning and distillation</strong>: combining techniques to maximise efficiency.</li><li><strong>Hardware support</strong>: new accelerators are emerging with native INT4 instructions.</li></ul><hr><h2 id=closing-thoughts>Closing thoughts<a hidden class=anchor aria-hidden=true href=#closing-thoughts>#</a></h2><p>Quantisation is a trade-off: you exchange some model quality for a smaller footprint and faster execution. For edge deployment, it often makes the difference between “impossible” and “usable”. With careful choice of bit width, calibration, and tools, it’s possible to run models that once needed datacentres directly on laptops or embedded devices.</p><hr></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://rossn1.github.io/posts/how-quantisation-works/><span class=title>« Prev</span><br><span>How Quantisation Works: From FP32 to INT4</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://rossn1.github.io/>ross.</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>