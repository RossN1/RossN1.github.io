<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>ross.</title><link>https://rossn1.github.io/</link><description>Recent content on ross.</description><generator>Hugo -- 0.150.0</generator><language>en-gb</language><lastBuildDate>Wed, 24 Sep 2025 11:00:00 +0100</lastBuildDate><atom:link href="https://rossn1.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Why Quantisation Matters for LLMs</title><link>https://rossn1.github.io/posts/why-quantisation-matters-for-llms/</link><pubDate>Wed, 24 Sep 2025 11:00:00 +0100</pubDate><guid>https://rossn1.github.io/posts/why-quantisation-matters-for-llms/</guid><description>&lt;p&gt;Large language models (LLMs) are powerful but expensive to run. A model with 7 billion parameters stored in FP32 requires around 28 GB of memory just for weights (7e9 Ã— 4 bytes). For larger models such as LLaMA-70B, this rises into the hundreds of gigabytes. These numbers explain why LLMs usually run in datacentres on specialised GPUs with high memory bandwidth.&lt;/p&gt;
&lt;p&gt;If we want to deploy models on consumer hardware or edge devices, these requirements are too high. Laptops and phones rarely have more than a few gigabytes available for AI tasks, and embedded systems often have less.&lt;/p&gt;</description></item><item><title>How Quantisation Works: From FP32 to INT4</title><link>https://rossn1.github.io/posts/how-quantisation-works/</link><pubDate>Wed, 24 Sep 2025 10:00:00 +0100</pubDate><guid>https://rossn1.github.io/posts/how-quantisation-works/</guid><description>&lt;p&gt;In the first post I explained why quantisation matters. Here Iâ€™ll focus on how it is applied, the main schemes in use, and some of the libraries that make it practical.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="precision-levels"&gt;Precision levels&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;FP32&lt;/strong&gt;: full precision, used in training. High accuracy, but memory-intensive.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FP16 / bfloat16&lt;/strong&gt;: half precision. Often the default for training and inference on GPUs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;INT8&lt;/strong&gt;: 8-bit integers. Reduces memory by 4Ã— compared to FP32.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;INT4&lt;/strong&gt;: 4-bit integers. More aggressive, but often still usable for inference.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each step down saves memory and bandwidth but increases quantisation error.&lt;/p&gt;</description></item><item><title>Deploying Quantised LLMs</title><link>https://rossn1.github.io/posts/deploying-quantised-llms/</link><pubDate>Wed, 24 Sep 2025 09:00:00 +0100</pubDate><guid>https://rossn1.github.io/posts/deploying-quantised-llms/</guid><description>&lt;p&gt;In the first two posts, I explained why quantisation matters and how it works. Here Iâ€™ll cover how to deploy quantised large language models (LLMs) on edge hardware, what workflows are available, and what to measure in practice.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="deployment-workflows"&gt;Deployment workflows&lt;/h2&gt;
&lt;h3 id="1-hugging-face-transformers--bitsandbytes-gpu-or-desktop"&gt;1. Hugging Face Transformers + bitsandbytes (GPU or desktop)&lt;/h3&gt;
&lt;p&gt;This is the most common path if you have a consumer GPU. &lt;code&gt;bitsandbytes&lt;/code&gt; integrates with Transformers to load models directly in 8-bit or 4-bit precision.&lt;/p&gt;</description></item><item><title>About</title><link>https://rossn1.github.io/about/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://rossn1.github.io/about/</guid><description>&lt;p&gt;Hi, Iâ€™m &lt;strong&gt;Ross.&lt;/strong&gt; ðŸ‘‹&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m an MSc Data Science graduate with an interest in artificial inteligence, machine learning, data analytics and software development.&lt;/p&gt;
&lt;p&gt;This blog is where I share notes, tutorials, and experiments.&lt;/p&gt;</description></item></channel></rss>